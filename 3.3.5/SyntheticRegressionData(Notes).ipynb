{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random \n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticRegressionData(d2l.DataModule):\n",
    "    \"\"\"Synthetic data for linear regression.\"\"\"\n",
    "    def __init__(self, w, b, noise=0.01, num_train=1000, num_val=1000,\n",
    "                 batch_size = 32):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        n = num_train + num_val\n",
    "        self.X  = torch.randn(n, len(w))\n",
    "        noise = torch.randn(n,1) *noise\n",
    "        self.y = torch.matmul(self.X, w.reshape((-1,1))) + b + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `super().__init__()`:\n",
    "The `super().__init__()` function is used to call the initializer (`__init__`) of the parent class. In your case, the class `SyntheticRegressionData` inherits from `d2l.DataModule`. So, `super().__init__()` calls the `__init__()` method of `d2l.DataModule`, allowing the class to initialize any attributes or methods that are defined in the parent class.(The parent class's __init__() is never called automatically when overriding it in the subclass. The subclass must explicitly call it using super() to invoke it.)\n",
    "- `torch.rand()` generates random numbers uniformly distributed in the interval [0,1)\n",
    "- `self.X = torch.randn(n, len(w))`\n",
    "This line generates a feature matrix `X` with `n` rows and `len(w)` columns, using the `torch.randn()` function, which creates a tensor filled with random values sampled from a normal distribution (mean 0, standard deviation 1). \n",
    "`self.X` will store the input features for your synthetic data.\n",
    "\n",
    "- `noise = torch.randn(n,1) * noise`\n",
    "The torch.rand() function generates values from a uniform distribution in the range [0, 1),\n",
    "FOR a different standard deviation and mean,\n",
    "```python\n",
    "mean = desired_mean   # deviated from zero\n",
    "std_dev = desired_std_dev_scale\n",
    "noise = torch.randn(n, 1) * std_dev + mean_from_zero\n",
    "```\n",
    "\n",
    "- `self.y = torch.matmul(self.X, w.reshape((-1,1))) + b + noise`\n",
    "This line calculates the target values `y` for the synthetic linear regression problem:\n",
    "`torch.matmul(self.X, w.reshape((-1,1)))`: The matrix multiplication between the feature matrix `self.X` and the reshaped weight vector `w`. Here, `w` is reshaped to be a column vector (having dimensions `len(w) x 1`) to match the matrix dimensions required for multiplication.\n",
    "So, the formula for `y` is essentially:\n",
    "\\[\n",
    "y = X \\cdot w + b + \\text{noise}\n",
    "\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: tensor([-0.7909, -0.5829]) \n",
      "label: tensor([4.6064])\n"
     ]
    }
   ],
   "source": [
    "data = SyntheticRegressionData(w=torch.tensor([2, -3.4]), b=4.2)\n",
    "print('features:', data.X[0], '\\nlabel:', data.y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([32, 2]) \n",
      "y shape: torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "@d2l.add_to_class(SyntheticRegressionData)\n",
    "def get_dataloader(self, train):\n",
    "    if train:\n",
    "        indices = list(range(0, self.num_train))\n",
    "        random.shuffle(indices) #example\n",
    "    else:\n",
    "        indices =list(range(self,num_train, self.num_train_self.num_val))\n",
    "    for i in range(0, len(indices), self.batch_size):\n",
    "        batch_indices = torch.tensor(indices[i: i+self.batch_size])\n",
    "        yield self.X[batch_indices], self.y[batch_indices]\n",
    "\n",
    "X, y = next(iter(data.train_dataloader()))\n",
    "print('X shape:', X.shape, '\\ny shape:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It divides the `num_train` (batch_size) into minibatches using `for i in range(0, len(indices), self.batch_size):` , grabbing one minibatch of examples at a time.\n",
    "\n",
    "It takes a batch size, a matrix of features, and a vector of labels, and generates minibatches of size batch_size. As such, each minibatch consists of a tuple of features and labels. Note that we need to be mindful of whether weâ€™re in training or validation mode: in the former, we will want to read the data in random order, whereas for the latter, being able to read data in a pre-defined order may be important for debugging purposes.\n",
    "\n",
    "While the iteration implemented above is good for didactic purposes, it is inefficient in ways that might get us into trouble with real problems. For example, `it requires that we load all the data in memory and that we perform lots of random memory access`. The built-in iterators implemented in a deep learning framework are considerably more efficient and they can deal with sources such as data stored in files, data received via a stream, and data generated or processed on the fly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concise Implementation of the Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([32, 2]) \n",
      "y shape: torch.Size([32, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@d2l.add_to_class(d2l.DataModule)  #@save\n",
    "def get_tensorloader(self, tensors, train, indices=slice(0, None)):\n",
    "    tensors = tuple(a[indices] for a in tensors)\n",
    "    dataset = torch.utils.data.TensorDataset(*tensors)\n",
    "    return torch.utils.data.DataLoader(dataset, self.batch_size, shuffle=train)\n",
    "\n",
    "@d2l.add_to_class(SyntheticRegressionData)  #@save\n",
    "def get_dataloader(self, train):\n",
    "    i = slice(0, self.num_train) if train else slice(self.num_train, None)\n",
    "    return self.get_tensorloader((self.X, self.y), train, i)\n",
    "\n",
    "X, y = next(iter(data.train_dataloader()))\n",
    "print('X shape:', X.shape, '\\ny shape:', y.shape)\n",
    "len(data.train_dataloader())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This DataLoader has been implemented in the `d2l.torch` library. It is more efficient and has some added functionality.\n",
    "Data loaders can be different for different data processing pipeline, that's why `get_dataloader` function is not defined in d2l.torch.DataModule rather defined inside `SyntheticRegressionData` which is a commonly used case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: tensor([1.8423, 0.5189]) \n",
      "label: tensor([6.1095])\n"
     ]
    }
   ],
   "source": [
    "class SyntheticRegressionData_onfly(d2l.DataModule):\n",
    "    \"\"\"Synthetic data for linear regression.\"\"\"\n",
    "    def __init__(self, w, b, noise=0.01, num_train=1000, num_val=1000,\n",
    "                 batch_size = 32, seed:int = 0):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        n = num_train + num_val\n",
    "        torch.manual_seed(seed)\n",
    "        self.X  = torch.randn(n, len(w))\n",
    "        noise = torch.randn(n,1) *noise\n",
    "        self.y = torch.matmul(self.X, w.reshape((-1,1))) + b + noise\n",
    "\n",
    "data = SyntheticRegressionData_onfly(w=torch.tensor([2, -3.4]), b=4.2, seed=5)\n",
    "print('features:', data.X[0], '\\nlabel:', data.y[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
