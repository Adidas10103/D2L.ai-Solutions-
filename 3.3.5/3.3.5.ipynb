{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParameters:\n",
    "    \"\"\"The base class of hyperparameters.\"\"\"\n",
    "    def save_hyperparameters(self, ignore=[]):\n",
    "        \"\"\"Defined in :numref:`sec_oo-design`\"\"\"\n",
    "        raise NotImplemented\n",
    "    def save_hyperparameters(self, ignore=[]):\n",
    "        \"\"\"Save function arguments into class attributes.\n",
    "        Defined in :numref:`sec_utils`\"\"\"\n",
    "        frame = inspect.currentframe().f_back\n",
    "        _, _, _, local_vars = inspect.getargvalues(frame)\n",
    "        self.hparams = {k:v for k, v in local_vars.items()\n",
    "                        if k not in set(ignore+['self']) and not k.startswith('_')}\n",
    "        for k, v in self.hparams.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "class DataModule(d2l.HyperParameters):\n",
    "    \"\"\"The base class of data.\n",
    "    Defined in :numref:`subsec_oo-design-models`\"\"\"\n",
    "    def __init__(self, root='../data', num_workers=4):\n",
    "        self.save_hyperparameters()\n",
    "    def get_dataloader(self, train):\n",
    "        raise NotImplementedError\n",
    "    def train_dataloader(self):\n",
    "        return self.get_dataloader(train=True)\n",
    "    def val_dataloader(self):\n",
    "        return self.get_dataloader(train=False)\n",
    "    def get_tensorloader(self, tensors, train, indices=slice(0, None)):\n",
    "        \"\"\"Defined in :numref:`sec_synthetic-regression-data`\"\"\"\n",
    "        tensors = tuple(a[indices] for a in tensors)\n",
    "        dataset = torch.utils.data.TensorDataset(*tensors)\n",
    "        return torch.utils.data.DataLoader(dataset, self.batch_size,\n",
    "                                           shuffle=train)\n",
    "    \n",
    "class SyntheticRegressionData(d2l.DataModule):\n",
    "    \"\"\"Synthetic data for linear regression. \n",
    "    Defined in :numref:`sec_synthetic-regression-data`\"\"\"\n",
    "    def __init__(self, w, b, noise=0.01, num_train=1000, num_val=1000,\n",
    "                batch_size=32):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        n = num_train + num_val\n",
    "        self.X = d2l.randn(n, len(w))\n",
    "        noise = d2l.randn(n, 1) * noise\n",
    "        self.y = d2l.matmul(self.X, d2l.reshape(w, (-1, 1))) + b + noise\n",
    "    def get_dataloader(self, train):\n",
    "        \"\"\"Defined in :numref:`sec_synthetic-regression-data`\"\"\"\n",
    "        i = slice(0, self.num_train) if train else slice(self.num_train, None)\n",
    "        return self.get_tensorloader((self.X, self.y), train, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(d2l.DataModule)  #@save\n",
    "def get_tensorloader(self, tensors, train, indices=slice(0, None)):\n",
    "    tensors = tuple(a[indices] for a in tensors)\n",
    "    dataset = torch.utils.data.TensorDataset(*tensors)\n",
    "    return torch.utils.data.DataLoader(dataset, self.batch_size, shuffle=train)\n",
    "\n",
    "@d2l.add_to_class(SyntheticRegressionData)  #@save\n",
    "def get_dataloader(self, train):\n",
    "    i = slice(0, self.num_train) if train else slice(self.num_train, None)\n",
    "    return self.get_tensorloader((self.X, self.y), train, i)\n",
    "\n",
    "X, y = next(iter(data.train_dataloader()))\n",
    "print('X shape:', X.shape, '\\ny shape:', y.shape)\n",
    "len(data.train_dataloader())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([32, 2]) \n",
      "y shape: torch.Size([32, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@d2l.add_to_class(d2l.DataModule)  #@save\n",
    "def get_tensorloader(self, tensors, train, indices=slice(0, None)):\n",
    "    tensors = tuple(a[indices] for a in tensors)\n",
    "    dataset = torch.utils.data.TensorDataset(*tensors)\n",
    "    return torch.utils.data.DataLoader(dataset, self.batch_size, \n",
    "                                       shuffle=train, drop_last = True)\n",
    "\n",
    "@d2l.add_to_class(SyntheticRegressionData)  #@save\n",
    "def get_dataloader(self, train):\n",
    "    i = slice(0, self.num_train) if train else slice(self.num_train, None)\n",
    "    return self.get_tensorloader((self.X, self.y), train, i)\n",
    "\n",
    "X, y = next(iter(data.train_dataloader()))\n",
    "print('X shape:', X.shape, '\\ny shape:', y.shape)\n",
    "len(data.train_dataloader())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "@d2l.add_to_class(SyntheticRegressionData)  #@save\n",
    "def get_dataloader(self, train):\n",
    "    i = slice(0, self.num_train) if train else slice(self.num_train, None)\n",
    "    return self.get_tensorloader((self.X, self.y), train, i)\n",
    "```\n",
    "##### What is a Slice Object? \n",
    "- A slice object in Python is used to specify a range of indices for slicing sequences like lists, tuples, and arrays.\n",
    "Example, \n",
    "You can use slice objects to extract portions of sequences. For example, if you have a list my_list, you can use my_list[slice(0, 5)] to get the first five elements.\n",
    "- `None`: When None is provided as the stop index in a slice object, it means \"up to the end of the sequence\".\n",
    "\n",
    "The `get_dataloader()` then calls the `get_tensorloader()` function and passes the tuple (data.X, data,y) = (`self.X`,`self.y`), `train` parameter and the slicing object into it.\n",
    "Therefore, the `get_dataloader()` has to be called twice separately for training data and validation data. (since the get_tensorloader() takes only 1 slice object as input)\n",
    "\n",
    "```python\n",
    "@d2l.add_to_class(d2l.DataModule)  #@save\n",
    "def get_tensorloader(self, tensors, train, indices=slice(0, None)):\n",
    "    tensors = tuple(a[indices] for a in tensors)\n",
    "    dataset = torch.utils.data.TensorDataset(*tensors)\n",
    "    return torch.utils.data.DataLoader(dataset, self.batch_size, \n",
    "                                       shuffle=train, drop_last = True)\n",
    "```\n",
    "- default value for `indices` argument, `indices=slice(0, None)`\n",
    "if `indices` is not provided. (no slicing, therefore everything is put in the training data)\n",
    "\n",
    "- `dataset = torch.utils.data.TensorDataset(*tensors)`:\n",
    " Unpacking with `*` operator, torch.utils.data.TensorDataset expects one or more tensors. It creates a dataset object where each element is a tuple of the corresponding elements from the input tensors. \n",
    "\n",
    "- `return torch.utils.data.DataLoader(dataset, self.batch_size, shuffle=train, drop_last = True)` is an instance of the `DataLoader` class. This class is a PyTorch utility that provides an iterable over the dataset, yielding batches of data.(shuffle if train=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. If the dataset is too large to fit into memory, an \"out of memory\" error will occur. In such cases, only a portion of the data can be loaded, preventing complete shuffling of the entire dataset.\n",
    "\n",
    "##### Why is Shuffling of Data Important ? \n",
    "[https://datascience.stackexchange.com/a/24539]\n",
    "\n",
    "[https://datascience.stackexchange.com/a/24524]\n",
    "Shuffling data serves the purpose of reducing variance and making sure that models remain general and overfit less.\n",
    "\n",
    "The obvious case where you'd shuffle your data is if your data is sorted by their class/target. Here, you will want to shuffle to make sure that your training/test/validation sets are representative of the overall distribution of the data.\n",
    "\n",
    "For batch gradient descent, the same logic applies. The idea behind batch gradient descent is that by calculating the gradient on a single batch, you will usually get a fairly good estimate of the \"true\" gradient. That way, you save computation time by not having to calculate the \"true\" gradient over the entire dataset every time.\n",
    "\n",
    "You want to shuffle your data(entire) after each epoch because you will always have the risk to create batches that are not representative of the overall dataset, and therefore, your estimate of the gradient will be off. Shuffling your data after each epoch ensures that you will not be \"stuck\" with too many bad batches.\n",
    "(epoch: An epoch is when all the training data is used at once and is defined as the total number of iterations of all the training data in one cycle for training the machine learning model.)\n",
    "\n",
    "In regular stochastic gradient descent, when each batch has size 1, you still want to shuffle your data after each epoch to keep your learning general. Indeed, if data point 17 is always used after data point 16, its own gradient will be biased with whatever updates data point 16 is making on the model. By shuffling your data, you ensure that each data point creates an \"independent\" change on the model, without being biased by the same points before them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ??\n",
    "[https://www.youtube.com/watch?v=onWTiaDx3Lw]\n",
    "\n",
    "[Apache Spark external sorting algorithm]\n",
    "\n",
    "[Apache Hadoop external sorting algorithm]\n",
    "\n",
    "[MapReduce: Simplified Data Processing on Large Clusters,Jeffrey Dean and Sanjay Ghemawat]\n",
    "\n",
    "[External Memory Algorithms and Data Structures:Dealing with Massive Data, JEFFREY SCOTT VITTER]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticRegressionData_onfly(d2l.DataModule):\n",
    "    \"\"\"Synthetic data for linear regression.\"\"\"\n",
    "    def __init__(self, w, b, noise=0.01, num_train=1000, num_val=1000,\n",
    "                 batch_size = 32):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        n = num_train + num_val\n",
    "        self.X  = torch.randn(n, len(w))\n",
    "        noise = torch.randn(n,1) *noise\n",
    "        self.y = torch.matmul(self.X, w.reshape((-1,1))) + b + noise\n",
    "\n",
    "data = SyntheticRegressionData(w=torch.tensor([2, -3.4]), b=4.2)\n",
    "print('features:', data.X[0], '\\nlabel:', data.y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "same as SyntheticRegressionData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: tensor([1.8423, 0.5189]) \n",
      "label: tensor([6.1095])\n"
     ]
    }
   ],
   "source": [
    "class SyntheticRegressionData(d2l.DataModule):\n",
    "    \"\"\"Synthetic data for linear regression.\"\"\"\n",
    "    def __init__(self, w, b, noise=0.01, num_train=1000, num_val=1000,\n",
    "                 batch_size = 32, seed:int = 0):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        n = num_train + num_val\n",
    "        torch.manual_seed(seed)\n",
    "        self.X  = torch.randn(n, len(w))\n",
    "        noise = torch.randn(n,1) *noise\n",
    "        self.y = torch.matmul(self.X, w.reshape((-1,1))) + b + noise\n",
    "\n",
    "data = SyntheticRegressionData(w=torch.tensor([2, -3.4]), b=4.2, seed=5)\n",
    "print('features:', data.X[0], '\\nlabel:', data.y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.manual_seed(seed)` initializes the random number generator (RNG) with a specific seed, ensuring that any subsequent random numbers generated will be deterministic and reproducible."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
